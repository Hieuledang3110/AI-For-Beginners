{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification with our own Framework\n",
    "\n",
    "Lab Assignment from [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).\n",
    "\n",
    "### Reading the Dataset\n",
    "\n",
    "This code download the dataset from the repository on the internet. You can also manually copy the dataset from `/data` directory of AI Curriculum repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  9.9M  100  9.9M    0     0   9.9M      0  0:00:01 --:--:--  0:00:01 15.8M\n"
     ]
    }
   ],
   "source": [
    "!rm *.pkl\n",
    "!wget https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz\n",
    "!gzip -d mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mnist.pkl','rb') as f:\n",
    "    MNIST = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST['Train']['Labels']\n",
    "data = MNIST['Train']['Features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the shape of data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "We will use Scikit Learn to split the data between training and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 33600, test samples: 8400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "print(f\"Train samples: {len(features_train)}, test samples: {len(features_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "1. Inter-layer activation function can both increase or decrease the network's performance. It may allow the network to break through soft peaks of the algorithm's accuracy, but it may also lead it into worse depths (without implementing backtracking)\n",
    "2. By simply brute-forcing a few hyperparameters within the network's gradient descent, the accuracy of the network can increase drastically already. Adding more layers did not increase the accuracy of the network's predictions. Thus, adding aditional perceptron layers are not always the solution into learning a system.\n",
    "3. As the number increase, the algorithm often converges to a set of weights as it deems to be the global max of the system when its only the local max. Further algorithms which allow the network to surpass local max can be implemented for possible higher accuracy.\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "def no_func(x):\n",
    "    \"\"\"No activation function.\"\"\"\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation function.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Load the MNIST data with the correct encoding\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as mnist_pickle:\n",
    "    MNIST = pickle.load(mnist_pickle, encoding='latin1')  # Specify encoding explicitly\n",
    "\n",
    "training_data, validation_data, test_data = MNIST  # Unpack the tuple\n",
    "# Access training features and labels\n",
    "train_features, train_labels = training_data\n",
    "val_features, val_labels = validation_data\n",
    "test_features, test_labels = test_data\n",
    "# Normalize features to the range [0, 1]\n",
    "train_features = train_features.astype(np.float32) / 255.0\n",
    "val_features = val_features.astype(np.float32) / 255.0\n",
    "test_features = test_features.astype(np.float32) / 255.0\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    \"\"\"Convert integer labels to one-hot encoded vectors.\"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Multi-layer Perceptron Weights Initialization\n",
    "def initialize_mlp(layers):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a multi-layer perceptron.\n",
    "    layers: List of layer sizes, e.g., [784, 128, 64, 10]\n",
    "    Returns:\n",
    "    - weights: List of weight matrices for each layer\n",
    "    - biases: List of bias vectors for each layer\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights.append(np.random.randn(layers[i], layers[i + 1]) * 0.01)\n",
    "        biases.append(np.zeros((1, layers[i + 1])))\n",
    "    return weights, biases\n",
    "\n",
    "# Forward pass through the network\n",
    "def forward_mlp(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Perform a forward pass through the MLP.\n",
    "    X: Input data (batch_size x input_size)\n",
    "    weights: List of weight matrices\n",
    "    biases: List of bias vectors\n",
    "    activation_fn: Activation function to apply\n",
    "    Returns:\n",
    "    - activations: List of activations for each layer\n",
    "    \"\"\"\n",
    "    activations = [X]\n",
    "    for l in range(len(weights)):\n",
    "        Z = np.dot(activations[-1], weights[l]) + biases[l]  # Weighted sum + bias\n",
    "        # Apply ReLU for hidden layers, softmax for the output layer\n",
    "        if l < len(weights) - 1:  # Hidden layers\n",
    "            A = relu(Z)\n",
    "        else:  # Output layer\n",
    "            A = softmax(Z)\n",
    "        activations.append(A)\n",
    "    return activations\n",
    "\n",
    "# Train function for a single layer\n",
    "def train_layer(X, y, weights, biases, num_iterations, lambda_reg, learning_rate, activation_fn=relu):\n",
    "    \"\"\"\n",
    "    Train a single layer of the MLP.\n",
    "    X: Input features\n",
    "    y: Target outputs\n",
    "    weights, biases: Current weights and biases for the layer\n",
    "    \"\"\"\n",
    "    for i in range(num_iterations):\n",
    "        idx = random.randint(0, X.shape[0] - 1)\n",
    "        x_sample = X[idx:idx + 1]  # Random single sample\n",
    "        y_sample = y[idx:idx + 1]  # Corresponding label\n",
    "\n",
    "        # Forward pass through the layer\n",
    "        Z = np.dot(x_sample, weights) + biases\n",
    "        A = activation_fn(Z)\n",
    "\n",
    "        # Compute error (difference between prediction and target)\n",
    "        error = A - y_sample\n",
    "\n",
    "        # Update weights and biases using the error\n",
    "        weights -= learning_rate * np.dot(x_sample.T, error) + lambda_reg * weights\n",
    "        biases -= learning_rate * error\n",
    "\n",
    "        # # Debug: Print weight updates\n",
    "        # if i % 10 == 0:  # Every 10 iterations\n",
    "        #     print(f\"Iteration {i}, Weights Norm: {np.linalg.norm(weights):.4f}, Error Norm: {np.linalg.norm(error):.4f}\")\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# Train the entire network, simultaneously training hidden layers\n",
    "def train_mlp(train_features, train_labels, layers, num_iterations, lambda_reg, learning_rate):\n",
    "    weights, biases = initialize_mlp(layers)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Forward pass through all layers\n",
    "        activations = forward_mlp(train_features, weights, biases)\n",
    "        \n",
    "        # Compute error at the output layer\n",
    "        error = activations[-1] - train_labels\n",
    "        \n",
    "        # Update weights and biases for the output layer\n",
    "        weights[-1] -= learning_rate * np.dot(activations[-2].T, error) + lambda_reg * weights[-1]\n",
    "        biases[-1] -= learning_rate * np.mean(error, axis=0, keepdims=True)\n",
    "        \n",
    "        # # Debugging: Print loss every 10 iterations\n",
    "        # if i % 10 == 0:\n",
    "        #     loss = -np.mean(train_labels * np.log(activations[-1] + 1e-9))  # Cross-entropy loss\n",
    "        #     print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# Classification through the network\n",
    "def classify_mlp(test_features, weights, biases):\n",
    "    \"\"\"\n",
    "    Classify samples using the trained MLP.\n",
    "    test_features: Input features\n",
    "    weights, biases: Trained weights and biases\n",
    "    \"\"\"\n",
    "    activations = forward_mlp(test_features, weights, biases)\n",
    "    output_layer = activations[-1]  # Softmax output from the final layer\n",
    "    \n",
    "    # Predicted class is the one with the highest probability\n",
    "    predicted_labels = np.argmax(output_layer, axis=1)\n",
    "    print(\"First 5 predictions (probabilities):\", output_layer[:5])\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions (probabilities): [[0.09877945 0.11195301 0.09939602 0.1016998  0.09752218 0.09155338\n",
      "  0.0991078  0.10301774 0.09722568 0.09974495]\n",
      " [0.09879936 0.11193573 0.09941009 0.10174123 0.09750016 0.09156223\n",
      "  0.09910821 0.10296829 0.09725476 0.09971994]\n",
      " [0.09876133 0.11198903 0.09939948 0.10170149 0.0975277  0.09154807\n",
      "  0.09910725 0.1030021  0.09721723 0.09974631]\n",
      " [0.0988173  0.11192094 0.09940208 0.10170825 0.09751097 0.09155885\n",
      "  0.09911603 0.10298894 0.09724541 0.09973122]\n",
      " [0.09878481 0.11194863 0.09939463 0.10169948 0.09752915 0.09155167\n",
      "  0.09910112 0.10300623 0.0972332  0.09975109]]\n",
      "Test accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "# View activation functions here: https://www.geeksforgeeks.org/activation-functions-neural-networks/\n",
    "\n",
    "# Define the architecture\n",
    "layers = [784, 256, 64, 10]  # Example: 2 hidden layers with 128 and 64 neurons\n",
    "\n",
    "# One-hot encode the labels for training\n",
    "train_labels_one_hot = one_hot_encode(train_labels)\n",
    "\n",
    "# Train the MLP\n",
    "weights, biases = train_mlp(\n",
    "    train_features,\n",
    "    train_labels_one_hot,\n",
    "    layers,\n",
    "    num_iterations=200,\n",
    "    lambda_reg=0.01,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "# Test the MLP\n",
    "predicted_labels = classify_mlp(test_features, weights, biases)\n",
    "accuracy = np.mean(predicted_labels == test_labels)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
