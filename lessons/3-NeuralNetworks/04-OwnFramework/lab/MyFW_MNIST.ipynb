{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification with our own Framework\n",
    "\n",
    "Lab Assignment from [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).\n",
    "\n",
    "### Reading the Dataset\n",
    "\n",
    "This code download the dataset from the repository on the internet. You can also manually copy the dataset from `/data` directory of AI Curriculum repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  9.9M  100  9.9M    0     0   9.9M      0  0:00:01 --:--:--  0:00:01 15.8M\n"
     ]
    }
   ],
   "source": [
    "!rm *.pkl\n",
    "!wget https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz\n",
    "!gzip -d mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mnist.pkl','rb') as f:\n",
    "    MNIST = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST['Train']['Labels']\n",
    "data = MNIST['Train']['Features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the shape of data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "We will use Scikit Learn to split the data between training and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 33600, test samples: 8400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "print(f\"Train samples: {len(features_train)}, test samples: {len(features_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "1. Inter-layer activation function can both increase or decrease the network's performance. It may allow the network to break through soft peaks of the algorithm's accuracy, but it may also lead it into worse depths (without implementing backtracking)\n",
    "2. By simply brute-forcing a few hyperparameters within the network's gradient descent, the accuracy of the network can increase drastically already. Adding more layers did not increase the accuracy of the network's predictions. Thus, adding aditional perceptron layers are not always the solution into learning a system.\n",
    "3. As the number increase, the algorithm often converges to a set of weights as it deems to be the global max of the system when its only the local max. Further algorithms which allow the network to surpass local max can be implemented for possible higher accuracy.\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "# Load the MNIST data with the correct encoding\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as mnist_pickle:\n",
    "    MNIST = pickle.load(mnist_pickle, encoding='latin1')  # Specify encoding explicitly\n",
    "\n",
    "training_data, validation_data, test_data = MNIST  # Unpack the tuple\n",
    "# Access training features and labels\n",
    "train_features, train_labels = training_data\n",
    "val_features, val_labels = validation_data\n",
    "test_features, test_labels = test_data\n",
    "# Normalize features to the range [0, 1] to improve training stability\n",
    "# Note: This is a common practice for many machine learning algorithm. MNIST dataset contains grayscale images, where each pixel value ranges from 0 to 255\n",
    "train_features = train_features.astype(np.float32) / 255.0\n",
    "val_features = val_features.astype(np.float32) / 255.0\n",
    "test_features = test_features.astype(np.float32) / 255.0\n",
    "\n",
    "# Filter positive and negative examples for training\n",
    "def set_mnist_pos_neg(target_label, x_labels, x_features):\n",
    "    positive_indices = [i for i, label in enumerate(x_labels) if label == target_label]\n",
    "    negative_indices = [i for i, label in enumerate(x_labels) if label != target_label]\n",
    "\n",
    "    positive_images = x_features[positive_indices]\n",
    "    negative_images = x_features[negative_indices]\n",
    "    \n",
    "    return positive_images, negative_images\n",
    "\n",
    "# Train function for a single binary classifier (one-vs-all)\n",
    "def train(positive_examples, negative_examples, num_iterations, lambda_reg, weights, activation_func):\n",
    "    num_dims = positive_examples.shape[1]  # Number of features\n",
    "    if weights is None:  # Initialize weights if not provided\n",
    "        weights = np.zeros((num_dims, 1))*0.01 # Shape: (num_features, 1), initialized with small values to prevent convergence issues\n",
    "\n",
    "    for i in range(num_iterations):  # Optimize weights through gradient descent\n",
    "        pos = random.choice(positive_examples).reshape(-1, 1)  # Shape: (num_features, 1)\n",
    "        neg = random.choice(negative_examples).reshape(-1, 1)  # Shape: (num_features, 1)\n",
    "\n",
    "        # Apply the activation function to the weighted sum (dot product)\n",
    "        pos_output = activation_func(np.dot(weights.T, pos))\n",
    "        neg_output = activation_func(np.dot(weights.T, neg))\n",
    "\n",
    "        # Update weights based on activation outputs\n",
    "        if pos_output < 0:\n",
    "            weights += pos  # Update weights with positive example\n",
    "        if neg_output >= 0:\n",
    "            weights -= neg  # Update weights with negative example\n",
    "\n",
    "        # Apply L2 regularization\n",
    "        weights -= lambda_reg * weights  # Regularization step\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Train one-vs-all classifiers for one layer with weight updates\n",
    "def train_all_classes(train_features, train_labels, num_iterations, lambda_reg, weights_list, activation_func):\n",
    "    if weights_list is None:\n",
    "        # Initialize weights for 10 classes if not provided\n",
    "        weights_list = [np.zeros((train_features.shape[1], 1)) for _ in range(10)]  \n",
    "\n",
    "    updated_weights_list = []\n",
    "    for digit in range(10):  # Train one classifier per digit (0-9)\n",
    "        pos_examples, neg_examples = set_mnist_pos_neg(digit, train_labels, train_features)\n",
    "        # Pass existing weights to the train function to update them\n",
    "        weights = train(pos_examples, neg_examples, num_iterations, lambda_reg, weights_list[digit], activation_func)\n",
    "        updated_weights_list.append(weights)\n",
    "\n",
    "    return updated_weights_list\n",
    "\n",
    "\n",
    "# Multi-layer perceptron with weight updating across layers\n",
    "def multi_layer_training(train_features, train_labels, num_iterations, lambda_reg, num_layers, activation_func):\n",
    "    weights_list = None  # Initialize weights list for the first layer\n",
    "    accuracies = []  # Track accuracies for each layer\n",
    "    for layer in range(num_layers):\n",
    "        print(f\"Training Layer {layer + 1}\")\n",
    "        weights_list = train_all_classes(train_features, train_labels, num_iterations, lambda_reg, weights_list, activation_func)\n",
    "        # Evaluate the current weights on the test set\n",
    "        predicted_labels = classify_multi_class(weights_list, test_features, test_labels)\n",
    "        layer_accuracy = accuracy(predicted_labels, test_labels)\n",
    "        accuracies.append(layer_accuracy)\n",
    "        print(f\"Accuracy after Layer {layer + 1}: {layer_accuracy:.4f}\")\n",
    "    return weights_list, accuracies\n",
    "\n",
    "\n",
    "# Classification Function with Matrix Multiplication\n",
    "def classify_multi_class(weights_list, test_features, test_labels):\n",
    "    # Convert weights list to a (num_features, 10) matrix\n",
    "    weights_matrix = np.column_stack(weights_list)  # Shape: (num_features, 10)\n",
    "    # Compute scores: test_features (num_samples, num_features) * weights_matrix (num_features, 10)\n",
    "    scores = np.dot(test_features, weights_matrix)  # Shape: (num_samples, 10)\n",
    "    # For each test sample, pick the class with the highest score\n",
    "    predicted_labels = np.argmax(scores, axis=1)  # Shape: (num_samples, ) - max score index for each sample\n",
    "    return predicted_labels\n",
    "\n",
    "def accuracy(predicted_labels, test_labels):\n",
    "    return float(np.sum(predicted_labels == test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1\n",
      "Accuracy after Layer 1: 0.8149\n",
      "Training Layer 2\n",
      "Accuracy after Layer 2: 0.8248\n",
      "Training Layer 3\n",
      "Accuracy after Layer 3: 0.8256\n",
      "Training Layer 4\n",
      "Accuracy after Layer 4: 0.8258\n",
      "Training Layer 5\n",
      "Accuracy after Layer 5: 0.8258\n",
      "Training Layer 6\n",
      "Accuracy after Layer 6: 0.8258\n",
      "Training Layer 7\n",
      "Accuracy after Layer 7: 0.8258\n",
      "Training Layer 8\n",
      "Accuracy after Layer 8: 0.8258\n",
      "Training Layer 9\n",
      "Accuracy after Layer 9: 0.8258\n",
      "Training Layer 10\n",
      "Accuracy after Layer 10: 0.8258\n",
      "Final multi-layer perceptron accuracy: 0.8258\n"
     ]
    }
   ],
   "source": [
    "# View activation functions here: https://www.geeksforgeeks.org/activation-functions-neural-networks/\n",
    "# No activation function\n",
    "def no_activation(x):\n",
    "    return x\n",
    "    \n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Softmax activation function (used for output layer in multi-class classification)\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability improvement\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "# Train and evaluate the multi-layer perceptron\n",
    "num_layers = 10  # Define the number of layers\n",
    "weights_list, accuracies = multi_layer_training(train_features, train_labels, 200, 0.-0.015, num_layers, no_activation)\n",
    "# Final accuracy after all layers\n",
    "print(\"Final multi-layer perceptron accuracy:\", accuracies[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
